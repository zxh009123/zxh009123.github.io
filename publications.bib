
@misc{zhang_geodtr_2023,
	title = {{GeoDTR}+: {Toward} generic cross-view geolocalization via geometric disentanglement},
	shorttitle = {{GeoDTR}+},
	url = {http://arxiv.org/abs/2308.09624},
	abstract = {Cross-View Geo-Localization (CVGL) estimates the location of a ground image by matching it to a geo-tagged aerial image in a database. Recent works achieve outstanding progress on CVGL benchmarks. However, existing methods still suffer from poor performance in cross-area evaluation, in which the training and testing data are captured from completely distinct areas. We attribute this deficiency to the lack of ability to extract the geometric layout of visual features and models’ overfitting to low-level details. Our preliminary work [1] introduced a Geometric Layout Extractor (GLE) to capture the geometric layout from input features. However, the previous GLE does not fully exploit information in the input feature. In this work, we propose GeoDTR+ with an enhanced GLE module that better models the correlations among visual features. To fully explore the LS techniques from our preliminary work, we further propose Contrastive Hard Samples Generation (CHSG) to facilitate model training. Extensive experiments show that GeoDTR+ achieves state-of-the-art (SOTA) results in cross-area evaluation on CVUSA [2], CVACT [3], and VIGOR [4] by a large margin (16.44\%, 22.71\%, and 17.02\% without polar transformation) while keeping the same-area performance comparable to existing SOTA. Moreover, we provide detailed analyses of GeoDTR+.},
	language = {en},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Zhang, Xiaohan and Li, Xingyu and Sultani, Waqas and Chen, Chen and Wshah, Safwan},
	month = aug,
	year = {2023},
	note = {arXiv:2308.09624 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zhang_cross-view_2023,
	title = {Cross-{View} {Geo}-{Localization} via {Learning} {Disentangled} {Geometric} {Layout} {Correspondence}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/25457},
	doi = {10.1609/aaai.v37i3.25457},
	abstract = {Cross-view geo-localization aims to estimate the location of a query ground image by matching it to a reference geo-tagged aerial images database. As an extremely challenging task, its difficulties root in the drastic view changes and different capturing time between two views. Despite these difficulties, recent works achieve outstanding progress on cross-view geo-localization benchmarks. However, existing methods still suffer from poor performance on the cross-area benchmarks, in which the training and testing data are captured from two different regions. We attribute this deficiency to the lack of ability to extract the spatial configuration of visual feature layouts and models' overfitting on low-level details from the training set. In this paper, we propose GeoDTR which explicitly disentangles geometric information from raw features and learns the spatial correlations among visual features from aerial and ground pairs with a novel geometric layout extractor module. This module generates a set of geometric layout descriptors, modulating the raw features and producing high-quality latent representations. In addition, we elaborate on two categories of data augmentations, (i) Layout simulation, which varies the spatial configuration while keeping the low-level details intact. (ii) Semantic augmentation, which alters the low-level details and encourages the model to capture spatial configurations. These augmentations help to improve the performance of the cross-view geo-localization models, especially on the cross-area benchmarks. Moreover, we propose a counterfactual-based learning process to benefit the geometric layout extractor in exploring spatial information. Extensive experiments show that GeoDTR not only achieves state-of-the-art results but also significantly boosts the performance on same-area and cross-area benchmarks. Our code can be found at https://gitlab.com/vail-uvm/geodtr.},
	language = {en},
	number = {3},
	urldate = {2024-08-13},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Xiaohan and Li, Xingyu and Sultani, Waqas and Zhou, Yi and Wshah, Safwan},
	month = jun,
	year = {2023},
	note = {Number: 3},
	keywords = {CV: Applications},
	pages = {3480--3488},
}


@article{wilson_object_2022,
	title = {Object {Tracking} and {Geo}-{Localization} from {Street} {Images}},
	volume = {14},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/11/2575},
	doi = {10.3390/rs14112575},
	abstract = {Object geo-localization from images is crucial to many applications such as land surveying, self-driving, and asset management. Current visual object geo-localization algorithms suffer from hardware limitations and impractical assumptions limiting their usability in real-world applications. Most of the current methods assume object sparsity, the presence of objects in at least two frames, and most importantly they only support a single class of objects. In this paper, we present a novel two-stage technique that detects and geo-localizes dense, multi-class objects such as trafﬁc signs from street videos. Our algorithm is able to handle low frame rate inputs in which objects might be missing in one or more frames. We propose a detector that is not only able to detect objects in images, but also predicts a positional offset for each object relative to the camera GPS location. We also propose a novel tracker algorithm that is able to track a large number of multi-class objects. Many current geo-localization datasets require specialized hardware, suffer from idealized assumptions not representative of reality, and are often not publicly available. In this paper, we propose a public dataset called ARTSv2, which is an extension of ARTS dataset that covers a diverse set of roads in widely varying environments to ensure it is representative of real-world scenarios. Our dataset will both support future research and provide a crucial benchmark for the ﬁeld.},
	language = {en},
	number = {11},
	urldate = {2024-08-14},
	journal = {Remote Sensing},
	author = {Wilson, Daniel and Alshaabi, Thayer and Van Oort, Colin and Zhang, Xiaohan and Nelson, Jonathan and Wshah, Safwan},
	month = may,
	year = {2022},
	pages = {2575},
}

@article{wilson_image_2024,
	title = {Image and {Object} {Geo}-{Localization}},
	volume = {132},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-023-01942-3},
	doi = {10.1007/s11263-023-01942-3},
	abstract = {The concept of geo-localization broadly refers to the process of determining an entity’s geographical location, typically in the form of Global Positioning System (GPS) coordinates. The entity of interest may be an image, a sequence of images, a video, a satellite image, or even objects visible within the image. Recently, massive datasets of GPS-tagged media have become available due to smartphones and the internet, and deep learning has risen to prominence and enhanced the performance capabilities of machine learning models. These developments have enabled the rise of image and object geo-localization, which has impacted a wide range of applications such as augmented reality, robotics, self-driving vehicles, road maintenance, and 3D reconstruction. This paper provides a comprehensive survey of visual geo-localization, which may involve either determining the location at which an image has been captured (image geo-localization) or geolocating objects within an image (object geo-localization). We will provide an in-depth study of visual geo-localization including a summary of popular algorithms, a description of proposed datasets, and an analysis of performance results to illustrate the current state of the field.},
	language = {en},
	number = {4},
	urldate = {2024-08-13},
	journal = {International Journal of Computer Vision},
	author = {Wilson, Daniel and Zhang, Xiaohan and Sultani, Waqas and Wshah, Safwan},
	month = apr,
	year = {2024},
	keywords = {Artificial Intelligence, Cross-view geo-localization, Deep learning, Geo-localization, Image geo-localization, Object geo-localization},
	pages = {1350--1392},
}

@inproceedings{zhang_lanepainter_2022,
	title = {{LanePainter}: {Lane} {Marks} {Enhancement} via {Generative} {Adversarial} {Network}},
	shorttitle = {{LanePainter}},
	url = {https://ieeexplore-ieee-org.ezproxy.uvm.edu/abstract/document/9956446},
	doi = {10.1109/ICPR56361.2022.9956446},
	abstract = {For safety purposes, understanding the quality of the lane marks is essential for advanced driving technologies and pavement road maintenance. In practice, the performance of existing lane detection algorithms [1]–[3] struggles with lane marks in poor conditions, especially in rural areas where the lane marks are less maintained. However, enhancing the quality of lane marks is still not thoroughly studied. In this research, we collected a new public dataset to benchmark the performance of lane detection algorithms on various lane marks conditions from different rural areas in the U.S. Also, this paper proposed LanePainter, a Generative Adversarial Network (GAN) based model, which simultaneously enhances and assesses lane marks quality in street view images. To increase the detectability of lane marks, LanePainter enhances degraded lane marks of street view images directly on image pixels without affecting the high-quality lane marks. Our experiments demonstrated a substantial increase in the performance of current lane detection algorithms on low-quality lane marks after enhancing the images using our proposed LanePainter algorithm. Also, pavement maintenance can take benefit from our classification assessment method to quickly and accurately locate the regions with low-quality lane marks.},
	urldate = {2024-08-13},
	booktitle = {2022 26th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Zhang, Xiaohan and Wshah, Safwan},
	month = aug,
	year = {2022},
	note = {ISSN: 2831-7475},
	keywords = {Benchmark testing, Classification algorithms, Generative adversarial networks, Lane detection, Maintenance engineering, Roads, Safety},
	pages = {3668--3675},
}

@inproceedings{liu_low-light_2022,
	title = {Low-light {Image} {Enhancement} {Using} {Chain}-consistent {Adversarial} {Networks}},
	url = {https://ieeexplore-ieee-org.ezproxy.uvm.edu/abstract/document/9956704},
	doi = {10.1109/ICPR56361.2022.9956704},
	abstract = {The capability to generate clear and bright images in low light situations is crucial for photographers, engineers, and researchers. When it is not possible to modify the imaging conditions, an algorithm to enhance images is needed. Traditional methods require manually adjusting parameters to tune the image. Supervised learning methods need to collect a large amount of paired data for training. In this paper, we demonstrate an semi-supervised method for low light image enhancement, using a chain of cycle consistent generators. We show the effectiveness of our method by comparing it to existing image enhancement methods, both using standard image quality metrics and by using human perceptual judgements. We include an ablation study for features in our model. Our proposed method is computationally efficient and does not require paired training data.},
	urldate = {2024-08-13},
	booktitle = {2022 26th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Liu, Minghao and Luo, Jiahao and Zhang, Xiaohan and Liu, Yang and Davis, James},
	month = aug,
	year = {2022},
	note = {ISSN: 2831-7475},
	keywords = {Generators, Image quality, Imaging, Measurement, Supervised learning, Training, Training data},
	pages = {713--719},
}

@misc{arrabi_cross-view_2024,
	title = {Cross-{View} {Meets} {Diffusion}: {Aerial} {Image} {Synthesis} with {Geometry} and {Text} {Guidance}},
	shorttitle = {Cross-{View} {Meets} {Diffusion}},
	url = {http://arxiv.org/abs/2408.04224},
	doi = {10.48550/arXiv.2408.04224},
	abstract = {Aerial imagery analysis is critical for many research fields. However, obtaining frequent high-quality aerial images is not always accessible due to its high effort and cost requirements. One solution is to use the Ground-to-Aerial (G2A) technique to synthesize aerial images from easily collectible ground images. However, G2A is rarely studied, because of its challenges, including but not limited to, the drastic view changes, occlusion, and range of visibility. In this paper, we present a novel Geometric Preserving Ground-to-Aerial (G2A) image synthesis (GPG2A) model that can generate realistic aerial images from ground images. GPG2A consists of two stages. The first stage predicts the Bird's Eye View (BEV) segmentation (referred to as the BEV layout map) from the ground image. The second stage synthesizes the aerial image from the predicted BEV layout map and text descriptions of the ground image. To train our model, we present a new multi-modal cross-view dataset, namely VIGORv2 which is built upon VIGOR with newly collected aerial images, maps, and text descriptions. Our extensive experiments illustrate that GPG2A synthesizes better geometry-preserved aerial images than existing models. We also present two applications, data augmentation for cross-view geo-localization and sketch-based region search, to further verify the effectiveness of our GPG2A. The code and data will be publicly available.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Arrabi, Ahmad and Zhang, Xiaohan and Sultan, Waqas and Chen, Chen and Wshah, Safwan},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04224 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ogilvie_fine-grained_2024,
	title = {Fine-{Grained} {Permeable} {Surface} {Mapping} through {Parallel} {U}-{Net}},
	volume = {24},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/24/7/2134},
	doi = {10.3390/s24072134},
	abstract = {Permeable surface mapping, which mainly is the identification of surface materials that will percolate, is essential for various environmental and civil engineering applications, such as urban planning, stormwater management, and groundwater modeling. Traditionally, this task involves labor-intensive manual classification, but deep learning offers an efficient alternative. Although several studies have tackled aerial image segmentation, the challenges in permeable surface mapping arid environments remain largely unexplored because of the difficulties in distinguishing pixel values of the input data and due to the unbalanced distribution of its classes. To address these issues, this research introduces a novel approach using a parallel U-Net model for the fine-grained semantic segmentation of permeable surfaces. The process involves binary classification to distinguish between entirely and partially permeable surfaces, followed by fine-grained classification into four distinct permeability levels. Results show that this novel method enhances accuracy, particularly when working with small, unbalanced datasets dominated by a single category. Furthermore, the proposed model is capable of generalizing across different geographical domains. Domain adaptation is explored to transfer knowledge from one location to another, addressing the challenges posed by varying environmental characteristics. Experiments demonstrate that the parallel U-Net model outperforms the baseline methods when applied across domains. To support this research and inspire future research, a novel permeable surface dataset is introduced, with pixel-wise fine-grained labeling for five distinct permeable surface classes. In summary, in this work, we offer a novel solution to permeable surface mapping, extend the boundaries of arid environment mapping, introduce a large-scale permeable surface dataset, and explore cross-area applications of the proposed model. The three contributions are enhancing the efficiency and accuracy of permeable surface mapping while progressing in this field.},
	language = {en},
	number = {7},
	urldate = {2024-08-13},
	journal = {Sensors},
	author = {Ogilvie, Nathaniel and Zhang, Xiaohan and Kochenour, Cale and Wshah, Safwan},
	month = jan,
	year = {2024},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {aerial imagery, arid environment, cross-domain adaptation, image segmentation, impervious surface mapping, permeable surface mapping, U-Net},
	pages = {2134},
}

@misc{wilson_visual_2023,
	title = {Visual and {Object} {Geo}-localization: {A} {Comprehensive} {Survey}},
	shorttitle = {Visual and {Object} {Geo}-localization},
	url = {http://arxiv.org/abs/2112.15202},
	doi = {10.48550/arXiv.2112.15202},
	abstract = {The concept of geo-localization refers to the process of determining where on earth some `entity' is located, typically using Global Positioning System (GPS) coordinates. The entity of interest may be an image, sequence of images, a video, satellite image, or even objects visible within the image. As massive datasets of GPS tagged media have rapidly become available due to smartphones and the internet, and deep learning has risen to enhance the performance capabilities of machine learning models, the fields of visual and object geo-localization have emerged due to its significant impact on a wide range of applications such as augmented reality, robotics, self-driving vehicles, road maintenance, and 3D reconstruction. This paper provides a comprehensive survey of geo-localization involving images, which involves either determining from where an image has been captured (Image geo-localization) or geo-locating objects within an image (Object geo-localization). We will provide an in-depth study, including a summary of popular algorithms, a description of proposed datasets, and an analysis of performance results to illustrate the current state of each field.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Wilson, Daniel and Zhang, Xiaohan and Sultani, Waqas and Wshah, Safwan},
	month = oct,
	year = {2023},
	note = {arXiv:2112.15202 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}